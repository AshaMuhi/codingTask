{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8YglsAarO23"
   },
   "source": [
    "# The power of neural networks\n",
    "\n",
    "Neurons can be used to model logic gates, the building blocks behind all digital computing. In this compulsory task we talk you through how to do so. We also explain how to represent neural networks in terms of matrix computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "nyr9_Tu6rO27"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'broadcast_to' from 'numpy.lib.stride_tricks' (C:\\Users\\Asha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\stride_tricks.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\__init__.py:980\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# When constructing the global instances, we need to perform certain updates\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# by explicitly calling the superclass (dict.update, dict.items) to avoid\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# triggering resolution of _auto_backend_sentinel.\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m rcParamsDefault \u001b[38;5;241m=\u001b[39m \u001b[43m_rc_params_in_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatplotlibrc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Strip leading comment.\u001b[39;49;00m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfail_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(rcParamsDefault, rcsetup\u001b[38;5;241m.\u001b[39m_hardcoded_defaults)\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# Normally, the default matplotlibrc file contains *no* entry for backend (the\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;66;03m# corresponding line starts with ##, not #; we fill on _auto_backend_sentinel\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# in that case.  However, packagers can set a different default backend\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# (resulting in a normal `#backend: foo` line) in which case we should *not*\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;66;03m# fill in _auto_backend_sentinel.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\__init__.py:914\u001b[0m, in \u001b[0;36m_rc_params_in_file\u001b[1;34m(fname, transform, fail_on_error)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m rcsetup\u001b[38;5;241m.\u001b[39m_validators:\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fail_on_error:\n\u001b[1;32m--> 914\u001b[0m         \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m val  \u001b[38;5;66;03m# try to convert to proper type or raise\u001b[39;00m\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    916\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\__init__.py:749\u001b[0m, in \u001b[0;36mRcParams.__setitem__\u001b[1;34m(self, key, val)\u001b[0m\n\u001b[0;32m    747\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 749\u001b[0m     cval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mve\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\rcsetup.py:342\u001b[0m, in \u001b[0;36mvalidate_color\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_color_like(stmp):\n\u001b[0;32m    340\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m stmp\n\u001b[1;32m--> 342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_color_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# If it is still valid, it must be a tuple (as a string from matplotlibrc).\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\colors.py:227\u001b[0m, in \u001b[0;36mis_color_like\u001b[1;34m(c)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     \u001b[43mto_rgba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\colors.py:309\u001b[0m, in \u001b[0;36mto_rgba\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    307\u001b[0m     rgba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rgba \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Suppress exception chaining of cache lookup failure.\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     rgba \u001b[38;5;241m=\u001b[39m \u001b[43m_to_rgba_no_colorcycle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m         _colors_full_map\u001b[38;5;241m.\u001b[39mcache[c, alpha] \u001b[38;5;241m=\u001b[39m rgba\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\colors.py:336\u001b[0m, in \u001b[0;36m_to_rgba_no_colorcycle\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be between 0 and 1, inclusive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    335\u001b[0m orig_c \u001b[38;5;241m=\u001b[39m c\n\u001b[1;32m--> 336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mis\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mma\u001b[49m\u001b[38;5;241m.\u001b[39mmasked:\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\__init__.py:357\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__dir__\u001b[39m():\n\u001b[0;32m    351\u001b[0m     public_symbols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m|\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m    352\u001b[0m     public_symbols \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    353\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatrixlib\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;66;03m# These were moved in 1.25 and may be deprecated eventually:\u001b[39;00m\n\u001b[0;32m    355\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModuleDeprecationWarning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisibleDeprecationWarning\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplexWarning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTooHardError\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAxisError\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 357\u001b[0m     }\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(public_symbols)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\__init__.py:42\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m=============\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mMasked Arrays\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extras\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndarray, amax, amin, iscomplexobj, bool_, _NoValue\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array \u001b[38;5;28;01mas\u001b[39;00m narray\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m angle\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     39\u001b[0m     getargspec, formatargspec, long, unicode, \u001b[38;5;28mbytes\u001b[39m\n\u001b[0;32m     40\u001b[0m     )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m expand_dims\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\function_base.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_newdoc\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtwodim_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m diag\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiarray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     _place, add_docstring, bincount, normalize_axis_index, _monotonicity,\n\u001b[0;32m     28\u001b[0m     interp \u001b[38;5;28;01mas\u001b[39;00m compiled_interp, interp_complex \u001b[38;5;28;01mas\u001b[39;00m compiled_interp_complex\n\u001b[0;32m     29\u001b[0m     )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mumath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _add_newdoc_ufunc \u001b[38;5;28;01mas\u001b[39;00m add_newdoc_ufunc\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\twodim_base.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m iinfo\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstride_tricks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m broadcast_to\n\u001b[0;32m     18\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiag\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiagflat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meye\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfliplr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflipud\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtri\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtriu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtril\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvander\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistogram2d\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask_indices\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtril_indices\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtril_indices_from\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtriu_indices\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtriu_indices_from\u001b[39m\u001b[38;5;124m'\u001b[39m, ]\n\u001b[0;32m     24\u001b[0m array_function_dispatch \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m     25\u001b[0m     overrides\u001b[38;5;241m.\u001b[39marray_function_dispatch, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'broadcast_to' from 'numpy.lib.stride_tricks' (C:\\Users\\Asha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\stride_tricks.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-mcTJnprO3I"
   },
   "source": [
    "## Neurons as logic gates\n",
    "\n",
    "A neuron works by applying an activation function, usually the sigmoid function, to a combination of inputs, input weights and a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ft6TavXAWSS0"
   },
   "outputs": [],
   "source": [
    "class Neuron():\n",
    "\n",
    "  def __init__(self, W, b):\n",
    "    self.W = W\n",
    "    self.b = b\n",
    "\n",
    "  def activate(self, X):\n",
    "    return sigmoid(W * X + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3pcg1o5XVuN"
   },
   "source": [
    "Here's a reminder of what the sigmoid function is and what it's output looks like:\n",
    "\n",
    "$$\n",
    "\\sigma = \\frac{1}{1 + e^{-x}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "executionInfo": {
     "elapsed": 617,
     "status": "ok",
     "timestamp": 1564935493778,
     "user": {
      "displayName": "Esther van den Berg",
      "photoUrl": "",
      "userId": "04126586416623518388"
     },
     "user_tz": -120
    },
    "id": "7WrC8RUprO3L",
    "outputId": "4abfec7a-65ec-4c5f-b5f7-dee85c7a37ba"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "inputs = np.arange(-100,100, step=0.1)\n",
    "plt.plot(inputs, sigmoid(inputs), linewidth=2)\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axvline(x=0, color='k')\n",
    "plt.xlim([-10, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqAZ7aCSrO3Z"
   },
   "source": [
    "### Logic gates\n",
    "\n",
    "A logic gate takes in two boolean inputs (0 or 1, i.e. True or False) and returns a single boolean output. An OR gate, for example, returns a 1 if either of the inputs is 1 or both are 1, and 0 only if both inputs are 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5fD9G7mX_vr"
   },
   "source": [
    "Can we design a neuron which produces the same outputs as an OR gate?\n",
    "\n",
    "In other words, can we find $w_1$, $w_2$ and $b$, such that $z$ in the following formula\n",
    "\n",
    "$$\n",
    "z = w_1 x_1 + w_2 x_2 + b\n",
    "$$\n",
    "\n",
    "corresponds to the outputs in the following OR gate truth table\n",
    "\n",
    "<table>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"3\">OR gate truth table</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"2\">Input</th>\n",
    "<th>Output</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtJTGjH-Y7_9"
   },
   "source": [
    "It turns out that we can.\n",
    "To make it easier to understand how, let's tease apart the weights and inputs of our neuron class to allow 2 inputs and 2 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "6cCOeexuXejJ"
   },
   "outputs": [],
   "source": [
    "class Neuron():\n",
    "\n",
    "  def __init__(self, w1, w2, b):\n",
    "    self.w1 = w1\n",
    "    self.w2 = w2\n",
    "    self.b = b\n",
    "\n",
    "  def activate(self, x1, x2):\n",
    "    return sigmoid(self.w1 * x1 + self.w2 * x2 + self.b)\n",
    "\n",
    "logic_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eF1cFmmWXts2"
   },
   "source": [
    "#### Bias\n",
    "\n",
    "The bias determines the value of $z$ if both inputs are 0.\n",
    "If both inputs are 0 we want the output to also be 0. So we must solve:\n",
    "\n",
    "$$\n",
    "0 =\\sigma(0 + 0 + b)\n",
    "$$\n",
    "\n",
    "\n",
    "The sigmoid function outputs values close to 0 if the input is about -7.5 or less, so $b$ must be at least that small. Let's specify $b$ to be -10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "rxD_KxHgZo3-"
   },
   "outputs": [],
   "source": [
    "b = -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyWWypZSZsYt"
   },
   "source": [
    "#### Weights\n",
    "\n",
    "The weights determine what happens when $x_1$ and/or $x_2$ are 1.\n",
    "In all the cases the output should be 1.\n",
    "\n",
    "The sigmoid function outputs about 1 for values larger than about 7.5, let's say 10. For either $w_1 + 0 + -10$ or $0 + w_1 + -10$ to be 10 or more, the weights must be at least 20.\n",
    "\n",
    "This also gives the correct output if both inputs are 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "YX-JkugzbAek"
   },
   "outputs": [],
   "source": [
    "w1, w2 = 20, 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1nC-I1WbEhk"
   },
   "source": [
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "executionInfo": {
     "elapsed": 487,
     "status": "ok",
     "timestamp": 1564936153442,
     "user": {
      "displayName": "Esther van den Berg",
      "photoUrl": "",
      "userId": "04126586416623518388"
     },
     "user_tz": -120
    },
    "id": "RWcOD1CFbFZB",
    "outputId": "bc6c8d8c-6bee-4bad-d31b-52191bce1082"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0: 0.0\n",
      "0, 1: 1.0\n",
      "1, 0: 1.0\n",
      "1, 1: 1.0\n"
     ]
    }
   ],
   "source": [
    "def make_truth_table(gate):\n",
    "  for x1, x2 in logic_inputs:\n",
    "    output = gate.activate(x1, x2)\n",
    "    print(\"{}, {}: {}\".format(x1, x2, np.round(output)))\n",
    "\n",
    "or_gate = Neuron(w1, w1, b)\n",
    "make_truth_table(or_gate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESl-j78arO3j"
   },
   "source": [
    "### Task 1.1\n",
    "Work out what values would model an AND gate for the neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKxZSnAQrO3i"
   },
   "source": [
    "<table>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"3\">AND gate truth table</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"2\">Input</th>\n",
    "<th>Output</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the values w1=1w_1 = 1w1​=1, w2=1w_2 = 1w2​=1, and b=−1.5b = -1.5b=−1.5 satisfy all the conditions, they correctly model an AND gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Gate Output:\n",
      "Input: (0, 0), Output: 0\n",
      "Input: (0, 1), Output: 0\n",
      "Input: (1, 0), Output: 0\n",
      "Input: (1, 1), Output: 1\n"
     ]
    }
   ],
   "source": [
    "def step_function(z):\n",
    "    return 1 if z >= 0 else 0\n",
    "\n",
    "def neuron_AND_gate(x1, x2):\n",
    "    # Define the weights and bias\n",
    "    w1 = 1\n",
    "    w2 = 1\n",
    "    b = -1.5\n",
    "    \n",
    "    # Calculate the weighted sum\n",
    "    z = w1 * x1 + w2 * x2 + b\n",
    "    \n",
    "    # Apply the step function\n",
    "    return step_function(z)\n",
    "\n",
    "# Test the neuron AND gate\n",
    "inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "print(\"AND Gate Output:\")\n",
    "for x1, x2 in inputs:\n",
    "    output = neuron_AND_gate(x1, x2)\n",
    "    print(f\"Input: ({x1}, {x2}), Output: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "executionInfo": {
     "elapsed": 494,
     "status": "ok",
     "timestamp": 1564936156837,
     "user": {
      "displayName": "Esther van den Berg",
      "photoUrl": "",
      "userId": "04126586416623518388"
     },
     "user_tz": -120
    },
    "id": "vgyObUnPrO3l",
    "outputId": "6258ae72-3784-47cc-f350-66e84aafa13e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Gate Truth Table:\n",
      " x1 | x2 | y \n",
      "----|----|----\n",
      " 0  | 0  | 0 \n",
      " 0  | 1  | 0 \n",
      " 1  | 0  | 0 \n",
      " 1  | 1  | 1 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Neuron:\n",
    "    def __init__(self, w1, w2, b):\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        self.b = b\n",
    "    \n",
    "    def step_function(self, z):\n",
    "        return 1 if z >= 0 else 0\n",
    "    \n",
    "    def output(self, x1, x2):\n",
    "        z = self.w1 * x1 + self.w2 * x2 + self.b\n",
    "        return self.step_function(z)\n",
    "\n",
    "def make_truth_table(neuron):\n",
    "    inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "    print(\"AND Gate Truth Table:\")\n",
    "    print(\" x1 | x2 | y \")\n",
    "    print(\"----|----|----\")\n",
    "    for x1, x2 in inputs:\n",
    "        y = neuron.output(x1, x2)\n",
    "        print(f\" {x1}  | {x2}  | {y} \")\n",
    "\n",
    "# Define the neuron with correct weights and bias for AND gate\n",
    "and_gate = Neuron(w1=1, w2=1, b=-1.5)\n",
    "\n",
    "# Generate and print the truth table\n",
    "make_truth_table(and_gate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TSxHQhOrO3o"
   },
   "source": [
    "### Task 1.2\n",
    "Do the same for the NOR gate and the NAND gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j25DAnwvrO3p"
   },
   "source": [
    "<table>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"3\">NOR gate truth table</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"2\">Input</th>\n",
    "<th>Output</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "z4mjHXntrO3p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Gate Truth Table:\n",
      " x1 | x2 | y \n",
      "----|----|----\n",
      " 0  | 0  | 1 \n",
      " 0  | 1  | 0 \n",
      " 1  | 0  | 0 \n",
      " 1  | 1  | 0 \n"
     ]
    }
   ],
   "source": [
    "#nor_gate = Neuron(w1=..., w2=..., b=...)\n",
    "\n",
    "make_truth_table(nor_gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Gate Truth Table:\n",
      " x1 | x2 | y \n",
      "----|----|----\n",
      " 0  | 0  | 1 \n",
      " 0  | 1  | 0 \n",
      " 1  | 0  | 0 \n",
      " 1  | 1  | 0 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the neuron with correct weights and bias for NOR gate\n",
    "nor_gate = Neuron(w1=-1, w2=-1, b=0.5)\n",
    "\n",
    "# Generate and print the truth table\n",
    "make_truth_table(nor_gate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IZ11iB5rO3r"
   },
   "source": [
    "<table>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"3\">NAND gate truth table</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"2\">Input</th>\n",
    "<th>Output</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "iAeI8G44rO3s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Gate Truth Table:\n",
      " x1 | x2 | y \n",
      "----|----|----\n",
      " 0  | 0  | 1 \n",
      " 0  | 1  | 1 \n",
      " 1  | 0  | 1 \n",
      " 1  | 1  | 0 \n"
     ]
    }
   ],
   "source": [
    "#nand_gate = Neuron(w1=..., w2=..., b=...)\n",
    "\n",
    "make_truth_table(nand_gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Gate Truth Table:\n",
      " x1 | x2 | y \n",
      "----|----|----\n",
      " 0  | 0  | 1 \n",
      " 0  | 1  | 1 \n",
      " 1  | 0  | 1 \n",
      " 1  | 1  | 0 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the neuron with correct weights and bias for NAND gate\n",
    "nand_gate = Neuron(w1=-1, w2=-1, b=1.5)\n",
    "\n",
    "# Generate and print the truth table\n",
    "make_truth_table(nand_gate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwaXX4DmrO3w"
   },
   "source": [
    "### The XOR Gate\n",
    "\n",
    "Of all logic gates the most important in computer science is the exclusive or or XOR gate.\n",
    "\n",
    "It turns out there is no configuration for our neuron that will replicate the XOR gate truth table.\n",
    "\n",
    "However, the XOR can be modeled by combining three of the gates we just made.  In other words,\n",
    "by combining several neurons into a network.\n",
    "\n",
    "See if you can find the combination of gates that produces this table:\n",
    "\n",
    "<table>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"3\">XOR gate truth table</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"2\">Input</th>\n",
    "<th>Output</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9q25E1v_ewLs"
   },
   "source": [
    "\n",
    "### Task 1.3\n",
    "\n",
    "Combine the gates we discussed. It's alright if you do so by trial and error. To help you out, the code below specifies that our combination\n",
    "first passes the inputs to two separate hidden gates or hidden neurons, and then passes the outcome of that to a single output neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Oe8f8HV1rO3x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Gate Truth Table:\n",
      " x1 | x2 | y \n",
      "----|----|----\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Network' object has no attribute 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_neuron\u001b[38;5;241m.\u001b[39mactivate(z1, z2)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#xor_gate = Network(..., ..., and_gate)\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mmake_truth_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxor_gate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 20\u001b[0m, in \u001b[0;36mmake_truth_table\u001b[1;34m(neuron)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----|----|----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x1, x2 \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[1;32m---> 20\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mneuron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m(x1, x2)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Network' object has no attribute 'output'"
     ]
    }
   ],
   "source": [
    "# Uncomment the xor_gate line and find out which neurons besides the or_gate neuron the\n",
    "# network should have in its hidden and output layer to produce the right values.\n",
    "\n",
    "class Network():\n",
    "\n",
    "  def __init__(self, gate1, gate2, out_gate):\n",
    "    self.hidden_neuron1 = gate1\n",
    "    self.hidden_neuron2 = gate2\n",
    "    self.out_neuron = out_gate\n",
    "\n",
    "  def activate(self, x1, x2):\n",
    "    z1 = self.hidden_neuron1.activate(x1, x2)\n",
    "    z2 = self.hidden_neuron2.activate(x1, x2)\n",
    "    return self.out_neuron.activate(z1, z2)\n",
    "\n",
    "#xor_gate = Network(..., ..., and_gate)\n",
    "make_truth_table(xor_gate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Define the Activation Functions   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Define the Neuron Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, weights, biases):\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "    \n",
    "    def activate(self, X):\n",
    "        self.z = np.dot(X, self.weights) + self.biases\n",
    "        self.a = sigmoid(self.z)\n",
    "        return self.a\n",
    "\n",
    "    def activate_single(self, x1, x2):\n",
    "        X = np.array([[x1, x2]])\n",
    "        return self.activate(X)[0, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Define the Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, gate1, gate2, out_gate):\n",
    "        self.hidden_neuron1 = gate1\n",
    "        self.hidden_neuron2 = gate2\n",
    "        self.out_neuron = out_gate\n",
    "\n",
    "    def activate(self, x1, x2):\n",
    "        z1 = self.hidden_neuron1.activate_single(x1, x2)\n",
    "        z2 = self.hidden_neuron2.activate_single(x1, x2)\n",
    "        return self.out_neuron.activate_single(z1, z2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights and biases for AND, NOR, and NAND gates\n",
    "# For AND gate\n",
    "W_and = np.array([[1], [1]])  # 2 inputs to 1 neuron\n",
    "b_and = np.array([[-1.5]])    # Bias for 1 neuron\n",
    "\n",
    "# For NOR gate\n",
    "W_nor = np.array([[-1], [-1]])  # 2 inputs to 1 neuron\n",
    "b_nor = np.array([[0.5]])       # Bias for 1 neuron\n",
    "\n",
    "# For NAND gate\n",
    "W_nand = np.array([[-1], [-1]])  # 2 inputs to 1 neuron\n",
    "b_nand = np.array([[1.5]])       # Bias for 1 neuron\n",
    "\n",
    "# Initialize the hidden and output layers\n",
    "and_gate = Layer(W_and, b_and)\n",
    "nor_gate = Layer(W_nor, b_nor)\n",
    "nand_gate = Layer(W_nand, b_nand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth Table:\n",
      " x1 | x2 | y \n",
      "----|----|----\n",
      " 0  | 0  | 0.0 \n",
      " 0  | 1  | 0.0 \n",
      " 1  | 0  | 0.0 \n",
      " 1  | 1  | 0.0 \n"
     ]
    }
   ],
   "source": [
    "# XOR gate using a combination of AND, NOR, and NAND gates\n",
    "xor_gate = Network(nand_gate, nor_gate, and_gate)\n",
    "\n",
    "# Function to generate and print the truth table\n",
    "def make_truth_table(network):\n",
    "    inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "    print(\"Truth Table:\")\n",
    "    print(\" x1 | x2 | y \")\n",
    "    print(\"----|----|----\")\n",
    "    for x1, x2 in inputs:\n",
    "        y = network.activate(x1, x2)\n",
    "        print(f\" {x1}  | {x2}  | {np.round(y)} \")\n",
    "\n",
    "# Generate and print the truth table for XOR gate\n",
    "make_truth_table(xor_gate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Gate Truth Table:\n",
      " x1 | x2 | y \n",
      "----|----|----\n",
      " 0  | 0  | 0 \n",
      " 0  | 1  | 0 \n",
      " 1  | 0  | 0 \n",
      " 1  | 1  | 1 \n",
      "\n",
      "NOR Gate Truth Table:\n",
      " x1 | x2 | y \n",
      "----|----|----\n",
      " 0  | 0  | 1 \n",
      " 0  | 1  | 0 \n",
      " 1  | 0  | 0 \n",
      " 1  | 1  | 0 \n",
      "\n",
      "NAND Gate Truth Table:\n",
      " x1 | x2 | y \n",
      "----|----|----\n",
      " 0  | 0  | 1 \n",
      " 0  | 1  | 1 \n",
      " 1  | 0  | 1 \n",
      " 1  | 1  | 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def make_truth_table(neuron, gate_name):\n",
    "    inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "    print(f\"{gate_name} Gate Truth Table:\")\n",
    "    print(\" x1 | x2 | y \")\n",
    "    print(\"----|----|----\")\n",
    "    for x1, x2 in inputs:\n",
    "        y = neuron.output(x1, x2)\n",
    "        print(f\" {x1}  | {x2}  | {y} \")\n",
    "    print()\n",
    "\n",
    "# Define the neurons with correct weights and biases for each gate\n",
    "and_gate = Neuron(w1=1, w2=1, b=-1.5)\n",
    "nor_gate = Neuron(w1=-1, w2=-1, b=0.5)\n",
    "nand_gate = Neuron(w1=-1, w2=-1, b=1.5)\n",
    "\n",
    "# Generate and print the truth tables\n",
    "make_truth_table(and_gate, \"AND\")\n",
    "make_truth_table(nor_gate, \"NOR\")\n",
    "make_truth_table(nand_gate, \"NAND\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hg2jOJBArO30"
   },
   "source": [
    "## Matrix Computations\n",
    "\n",
    "The code for a single neuron is fairly simple. When we combine neurons, however, the input is passed through multiple neurons in a hidden layer, which can be very large. The output of the hidden layer is itself either passed to more layers or an output layer of variable size. This can involve absolutely huge computations which are hard to understand and code efficiently.\n",
    "\n",
    "To understand these computations and work with neural network libraries, you must refresh your linear algebra and be able to think of networks in terms of matrix calculations. We'll warm you up with this gentle exercise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SELDEWQkcrq"
   },
   "source": [
    "### Input\n",
    "\n",
    "Instead of writing the input as seperate variables, we store each input as a vector and all inputs as a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h35ap3pRrO36"
   },
   "outputs": [],
   "source": [
    "logic_inputs = np.array(logic_inputs)\n",
    "logic_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpCii3ddk_nA"
   },
   "source": [
    "### Weights\n",
    "\n",
    "We do the same with weights.\n",
    "There are as many weight matrices as there are layers.\n",
    "Each cell $W_{i,j}$ in the matrix, where $i$ is the ith row and $j$ is the jth column, gives the weight from neuron $i$ in the previous (left) layer to neuron $j$ in the next (right) layer. In W,\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIV6j3talC9y"
   },
   "outputs": [],
   "source": [
    "# weights of the hidden layer of an OR gate\n",
    "W = np.array([[20],\n",
    "              [20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3AivjeisnPe"
   },
   "source": [
    "Instead of focusing on individual neurons, we focus on layers.\n",
    "We specify what size the input vectors ($m$) for the layer has, how many neurons ($n$) the layer has, and the bias for the layer.\n",
    "Instead of multipying each input with each neuron, we use np.dot to multiply the matrixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwmV63_msY2X"
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "\n",
    "  def __init__(self, W, b):\n",
    "    self.m = W.shape[0]\n",
    "    self.n = W.shape[1]\n",
    "    self.W = W\n",
    "    self.b = b\n",
    "\n",
    "  def activate(self, X):\n",
    "    z = np.dot(X, self.W) + self.b\n",
    "    return sigmoid(z)\n",
    "\n",
    "OR_layer = Layer(W1, -10)\n",
    "or_output = OR_layer.activate(X)\n",
    "np.round(or_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOV8eQhKwgCB"
   },
   "source": [
    "### Task 1.4\n",
    "\n",
    "Finish this version of an XOR gate that more closely resembles a neural network by determining the shapes the weights and biases need to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfuZR6OCwo2l"
   },
   "outputs": [],
   "source": [
    "#W1 = np.array(...)\n",
    "#b1 = np.array(...)\n",
    "\n",
    "#W2 = np.array(...)\n",
    "#b2 = np.array(...)\n",
    "\n",
    "hidden_layer = Layer(W1, b1)\n",
    "output_layer = Layer(W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rAs9CrJ7xG9_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Activation Functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define the Neuron Layers\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "    \n",
    "    def activate(self, X):\n",
    "        self.z = np.dot(X, self.weights) + self.biases\n",
    "        self.a = sigmoid(self.z)\n",
    "        return self.a\n",
    "\n",
    "# Define the Network Class\n",
    "class Network:\n",
    "    def __init__(self, hidden, output):\n",
    "        self.hidden = hidden\n",
    "        self.output = output\n",
    "\n",
    "    def activate(self, X):\n",
    "        z = self.hidden.activate(X)\n",
    "        return self.output.activate(z)\n",
    "\n",
    "# Initialize the hidden and output layers\n",
    "# Input size for XOR gate is 2\n",
    "hidden_layer = Layer(input_size=2, output_size=2)\n",
    "output_layer = Layer(input_size=2, output_size=1)\n",
    "\n",
    "# Create the XOR gate network\n",
    "xor_gate = Network(hidden=hidden_layer, output=output_layer)\n",
    "\n",
    "# Define the XOR input and output\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Training the Network\n",
    "epochs = 10000\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    hidden_output = hidden_layer.activate(X)\n",
    "    final_output = output_layer.activate(hidden_output)\n",
    "    \n",
    "    # Calculate error\n",
    "    output_error = y - final_output\n",
    "    output_delta = output_error * sigmoid_derivative(final_output)\n",
    "    \n",
    "    # Calculate error for hidden layer\n",
    "    hidden_error = output_delta.dot(output_layer.weights.T)\n",
    "    hidden_delta = hidden_error * sigmoid_derivative(hidden_output)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    output_layer.weights += hidden_output.T.dot(output_delta) * learning_rate\n",
    "    output_layer.biases += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
    "    hidden_layer.weights += X.T.dot(hidden_delta) * learning_rate\n",
    "    hidden_layer.biases += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "# Test the network after training\n",
    "xor_output = xor_gate.activate(X)\n",
    "print(np.round(xor_output))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Gate Output:\n",
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def activate(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, hidden, output):\n",
    "        self.hidden = hidden\n",
    "        self.output = output\n",
    "\n",
    "    def activate(self, X):\n",
    "        z = self.hidden.activate(X)\n",
    "        return self.output.activate(z)\n",
    "\n",
    "# Initialize weights and biases for XOR gate\n",
    "# Input to hidden layer weights (2 inputs to 2 neurons)\n",
    "hidden_weights = np.array([[1, 1], [1, 1]])\n",
    "hidden_bias = np.array([-1.5, 1.5])\n",
    "\n",
    "# Hidden to output layer weights (2 neurons to 1 output)\n",
    "output_weights = np.array([1, -2])\n",
    "output_bias = np.array([0.5])\n",
    "\n",
    "# Create layers\n",
    "hidden_layer = Layer(hidden_weights, hidden_bias)\n",
    "output_layer = Layer(output_weights, output_bias)\n",
    "\n",
    "# Create the network\n",
    "xor_gate = Network(hidden_layer, output_layer)\n",
    "\n",
    "# Define input for XOR gate\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# Get output\n",
    "xor_output = xor_gate.activate(X)\n",
    "rounded_output = np.round(xor_output)\n",
    "\n",
    "print(\"XOR Gate Output:\")\n",
    "print(rounded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
